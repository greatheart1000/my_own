class Args:
    train_path = './data/train_process.json'
    test_path = './data/test_process.json'
    seq_labels_path = './data/intents.txt'
    token_labels_path = './data/slots.txt'
    bert_dir = './model_hub/chinese-bert-wwm-ext/'
    save_dir = './checkpoints/'
    load_dir = './checkpoints/model.pt'
    do_train = True
    do_eval = False
    do_test = True
    do_save = True
    do_predict = True
    load_model = True
    device = None
    seqlabel2id = {}
    id2seqlabel = {}
    hidden_size = 768
    seq_num_labels = len(seq_labels)
    token_num_labels = len(tmp)
    max_len = 32
    batchsize = 64
    lr = 2e-5
    epoch = 10
    hidden_dropout_prob = 0.1

if __name__ == '__main__':
    args = Args()
    print(args.seq_labels)
    print(args.seqlabel2id)
    print("args.tokenlabel2id:",args.tokenlabel2id)
    print("args.nerlabel2id:",args.nerlabel2id)
    
#################################################

main.py
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
from torch.optim import Adam
import torch.nn as nn
import torch
import numpy as np
import os
from config import Args
from model import BertForIntentClassificationAndSlotFilling
from dataset import BertDataset
from preprocess import Processor, get_features

class Trainer:
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = Adam(self.model.parameters(), lr=config.lr)
        self.epoch = args.epoch
        self.device = args.device
    def train(self, train_loader):
        global_step = 0
        total_step = len(train_loader) * self.epoch
        self.model.train()
        for epoch in range(self.epoch):
            for step, train_batch in enumerate(train_loader):
                for key in train_batch.keys():
                    train_batch[key] = train_batch[key].to(self.device)
                input_ids = train_batch['input_ids']
                attention_mask = train_batch['attention_mask']
                token_type_ids = train_batch['token_type_ids']
                seq_label_ids = train_batch['seq_label_ids']
                token_label_ids = train_batch['token_label_ids']
                seq_output, token_output = self.model(
                    input_ids,
                    attention_mask,
                    token_type_ids,
                )

                active_loss = attention_mask.view(-1) == 1
                active_logits = token_output.view(-1, token_output.shape[2])[active_loss]
                active_labels = token_label_ids.view(-1)[active_loss]

                seq_loss = self.criterion(seq_output, seq_label_ids)
                token_loss = self.criterion(active_logits, active_labels)
                loss = seq_loss + token_loss
                self.model.zero_grad()
                loss.backward()
                self.optimizer.step()
                print(f'[train] epoch:{epoch+1} {global_step}/{total_step} loss:{loss.item()}')
                global_step += 1

        if self.config.do_save:
            self.save(self.config.save_dir, 'model.pt')

from torch.utils.data import DataLoader, Dataset

class BertDataset(Dataset):
    def __init__(self, features):
        self.features = features
        self.nums = len(self.features)

    def __len__(self):
        return self.nums

    def __getitem__(self, item):
        data = {
            'input_ids': self.features[item].input_ids.long(),
            'attention_mask': self.features[item].attention_mask.long(),
            'token_type_ids': self.features[item].token_type_ids.long(),
            'seq_label_ids': self.features[item].seq_label_ids.long(),
            'token_label_ids': self.features[item].token_label_ids.long(),
        }
        return data
